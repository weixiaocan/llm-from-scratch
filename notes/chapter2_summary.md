
# 第二章总结：分词与嵌入（Tokenizer & Embedding）

---

## 🧠 一、五问五答 · 关键知识点讲解

### 1. 什么是 Tokenizer？它的作用是什么？

Tokenizer 是大语言模型中的**分词器**，其作用是将原始文本（通常是一整段自然语言）**划分成更小的单元**，如字符、词或子词（subwords）。这样划分后的单位称为 Token。

每个 Token 会被映射到词表（Vocabulary）中的一个唯一编号，即 **Token ID**，这个编号是数字，可以作为模型的输入，便于后续的嵌入与训练处理。

---

### 2. BPE 分词是怎么工作的？它为什么被广泛使用？

BPE（Byte Pair Encoding）是一种常见的子词级分词算法。其主要流程是：

1. 初始时将所有文本拆成最小单位（如字符）；
2. 统计文本中频率最高的相邻字符对；
3. 将这个字符对合并为一个新的子词单位；
4. 反复执行合并操作，直到达到设定的词表大小。

BPE 的优点：

- 它能在词和字符之间取得平衡；
- **能处理未登录词**（out-of-vocabulary word）：将没见过的词分成多个见过的子词处理；
- 降低词表大小、提升泛化能力。

---

### 3. Token 和 Token ID 是什么关系？为什么要建立词表？

- **Token** 是经过分词处理得到的基本单元（如："我", "喜欢", "学习"）；
- **Token ID** 是它在词表中的对应编号，是一个整数。

构建词表的意义：

- 把自然语言变成模型能处理的数字形式；
- 使得每个 Token 都有唯一可识别的 ID；
- 是连接语言世界和模型世界的桥梁。

---

### 4. `nn.Embedding` 是怎么把 ID 映射成向量的？和 One-hot 有什么不同？

`nn.Embedding` 是一个嵌入层，用于将 Token ID 映射成稠密的向量表示。

- 输入：一个整数 ID（如 42）
- 操作：本质上是一个查表操作（Embedding 层是一个 [Vocab_Size, Hidden_Dim] 的矩阵）
- 输出：对应 ID 行的向量（如 `tensor([0.12, -0.55, ...])`）

和 One-hot 不同：

| 特性       | One-hot         | Embedding       |
|------------|------------------|------------------|
| 向量长度   | 词表大小         | 固定维度（如128） |
| 表示方式   | 只有一个位置为1   | 稠密向量         |
| 是否可训练 | ❌ 固定           | ✅ 可训练         |

---

### 5. 什么是位置编码？为什么需要它？它是怎么加进去的？

位置编码（Positional Encoding）是为了**在输入中注入序列顺序信息**。因为 Transformer 架构没有内建顺序感（不像 RNN），所以必须显式告诉模型每个 Token 的位置。

**它的重要性：**

- 让模型能区分“我爱你”和“你爱我”；
- 帮助捕捉语义中“位置依赖”的信息。

**加入方式如下：**

1. 首先通过 `nn.Embedding` 得到 token 的词向量表示：`x_embed`；
2. 构造一个 `pos_embed`（可以是可学习或 sin/cos 的位置向量）；
3. 使用加法将它们结合：

```python
x = x_embed + pos_embed
```

这样模型的每个输入向量就包含了“词的语义 + 位置信息”。

---


