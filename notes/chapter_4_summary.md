# Chapter 4 学习笔记：构建完整的 GPT 模型

### ❓ 五问五答

---

### 1️⃣ 为什么要使用层归一化（LayerNorm）？

为了**稳定训练过程**。

在深度神经网络中，数据在层与层之间传播时，其分布容易发生剧烈变化，导致**梯度爆炸或消失**，使模型难以收敛。层归一化（Layer Normalization）通过对每一层的输出进行归一化，使其均值为 0、方差为 1，从而：

- **保持数值稳定性**：防止数据尺度问题。
- **加快模型收敛**：使梯度更加平滑。
- **提高训练效率**：降低对初始化方法的敏感度。

---

### 2️⃣ 什么是快捷连接（残差连接），为什么需要它？

快捷连接（Shortcut / Residual Connection）是一种将前层输入直接“跳过”一层或多层，并加到后续层输出上的技术。

**表达式：** `output = Layer(x) + x`

它的核心作用是：

- **缓解梯度消失**：通过创建一条“绿色通道”，让梯度能够更直接地反向传播到浅层网络，从而有效训练更深的模型。
- **保留原始信息**：确保模型在进行复杂变换（`Layer(x)`）的同时，不会丢失原始输入（`x`）的信息。
- **提升信息流动效率**：让信息在网络中可以跨层传递。

---

### 3️⃣ 一个 Transformer Block 的构成是什么？

一个标准的 Transformer Block（GPT-2 采用的 Pre-Norm 结构）主要由以下模块按顺序组成：

1.  **层归一化 (LayerNorm)**
2.  **掩码多头注意力 (Masked Multi-Head Attention)**
3.  **残差连接 (Residual Connection)**
4.  **第二个层归一化 (LayerNorm)**
5.  **前馈神经网络 (Feed-Forward Network) + GELU 激活**
6.  **第二个残差连接 (Residual Connection)**

✅ 在第四章中，我们通过 `TransformerBlock` 类封装了这些组件。

---

### 4️⃣ GPT 模型的整体结构是怎样的？

GPT 是由多个 Transformer Block 堆叠而成的解码器（Decoder-only）架构。其结构大致如下：

```
Input Tokens
      ↓
词嵌入 (Token Embedding) + 位置编码 (Positional Encoding)
      ↓
[ Transformer Block ] × N (N个重复的块)
      ↓
最终层归一化 (Final LayerNorm)
      ↓
输出层 (Linear layer to vocabulary size)
      ↓
Logits (预测下一个词元的概率分布)
```

---

### 5️⃣ 为什么没有训练的 GPT 模型生成的是无意义文本？

因为模型参数是**随机初始化**的。在训练之前，这些权重完全不包含任何关于语言结构、语法、语义或世界知识的信息。

一个未经训练的模型：
- **无法识别词语组合**：不知道哪些词语搭配是合理的。
- **无法理解上下文**：不能根据前面的词预测下一个最可能的词。
- **输出是随机的**：其生成的内容只是基于随机权重的数学计算，因此结果是混乱且无意义的。

只有通过在海量文本数据上进行训练，模型才能调整参数，学习到语言的内在规律，从而生成连贯、有逻辑的文本。

---

### 总结与理解

在第四章中，我们从头实现了一个简化版的 GPT 模型。这个过程让我深刻理解了：

- **架构的重复性**：GPT 模型的核心是由标准化的 Transformer Block 堆叠而成。
- **分层特征学习**：每个 Block 负责在不同层次上学习上下文特征。
- **核心组件协同**：注意力机制、非线性变换和残差连接共同构建了强大的序列建模能力。
- **训练的决定性**：模型结构只是一个骨架，真正的“智能”完全来源于在数据上的训练过程。