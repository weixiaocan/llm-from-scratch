
# 📘 第三章：注意力机制（Attention）

## ❓五个关键问题与回答

1. **什么是注意力机制？它解决了 NLP 中的什么问题？**

注意力机制是在处理一段输入序列时，让模型能够根据每个词与其它词的关系来分配权重，从而聚焦于对当前任务最有帮助的信息。在 NLP 中，它主要用于解决长文本中远距离词语间的依赖问题，帮助模型理解上下文之间的深层关联。

---

2. **缩放点积注意力（Scaled Dot-Product Attention）是如何计算的？涉及哪些关键张量？**

在计算注意力得分时，会对 `Q`（Query）和 `K`（Key）做点积操作，然后将结果除以 `√dk`（Key 的维度），再进行 Softmax 得到注意力权重。这个缩放操作的目的是防止在高维情况下，点积结果过大，导致梯度消失或梯度爆炸。

---

3. **什么是 mask？它有哪几种类型？作用是什么？**

Mask 是用来控制注意力机制在计算时“看到”哪些位置的技术。

- 第一种类型是 **下三角矩阵（1/0 掩码）**，用于训练阶段，只允许注意力关注当前词及其之前的词。
- 第二种类型是 **负无穷填充型**，在 Attention 的 logits（得分）中将无效位置填为 `-inf`，Softmax 后结果趋近于 0，用于更精细的控制。

这两种 mask 的实现逻辑不同，但目标都是为了限制未来信息的泄露。

---

4. **什么是多头注意力机制？它相比单头有什么优势？**

多头注意力机制是将 Q/K/V 拆分成多个子空间（多个头），每个子空间独立计算注意力，最后再拼接并投影。

它的优势在于：
- 单一注意力头只能关注一种语言特征；
- 多个注意力头可以并行学习不同的语义信息（例如：词性、句法、情感倾向等）；
- 提升模型捕捉复杂结构和上下文的能力。

---



## 🧠 我的理解总结

注意力机制的核心思想是：“每个词的含义来自于它与其它词的关系”。它是 NLP 模型由“线性序列”走向“图式连接”的核心步骤，也是实现 Transformer、LLM 等模型的基石。理解了 Attention，后面学 Multi-head、Self-Attention、Transformer 架构就会轻松很多。

注意力机制的核心在于 点积注意力（Dot-Product Attention），通过计算每个词的 Query 与其他词的 Key 的相似度（点积），再用 Softmax 得到归一化的权重，并将这些权重作用于对应的 Value。整个过程就是：
Attention(Q, K, V) = softmax(QKᵀ / √dk) × V
其中的缩放项 √dk 是防止梯度爆炸。这个公式在概念上像是“让模型自己决定要注意哪些词”，数学上是一个加权平均。

注意力在信息流中的作用：在序列建模中，传统模型（如 RNN）只能逐步处理序列，难以建模远距离依赖。而 Attention 机制可以在一次并行计算中，让每个词都感知到其它所有词的上下文，并通过学习得到的权重决定“该关注谁”。这大大增强了模型的信息提取能力，也提高了训练效率。

---


